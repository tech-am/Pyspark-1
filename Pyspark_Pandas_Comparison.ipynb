{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import findspark\n",
    "findspark.init('/opt/spark')\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql.types import StructField,IntegerType, StructType,StringType, FloatType\n",
    "from pyspark.sql.functions import desc \n",
    "from pyspark.sql.functions import asc \n",
    "import pyspark.sql.functions as F \n",
    "import numpy as np\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling: Pandas vs. Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic introduction is to compare common data wrangling methods in pyspark and pandas data frame with a concrete example. Here, I used US counties' COVID-19 dataset to show the data wrangling differences between these two types of data frames. The dataset can be downloaded from Kaggle Dataset (https://www.kaggle.com/fireballbyedimyrnmom/us-counties-covid-19-dataset). This should allow you to get started with data manipulation and analysis under both pandas and spark. Specific objectives are to show you how to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load data from local files\n",
    "2. Display the schema of the DataFrame \n",
    "3.\tChange data types of the DataFrame \n",
    "4.  Show the head of the DataFrame \n",
    "4.\tSelect columns from the DataFrame \n",
    "5.\tShow the statistics of the DataFrame \n",
    "6.\tDrop duplicates \n",
    "7.\tMissing Values (check NA, drop NA, replace NA)\n",
    "8.\tDatetime manipulations\n",
    "9.\tFilter data based on conditions \n",
    "10.\tAggregation functions\n",
    "12.\tSort values \n",
    "13.\tRename columns\n",
    "14.\tCreate new columns \n",
    "15.\tJoin tables \n",
    "16.\tUser Defined functions \n",
    "17.\tWindow function \n",
    "18.\tOperate SQL queries within DataFrame \n",
    "19.\tConvert one type of DataFrame to another \n",
    "20.\tWrite out the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\tLoad data from local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike pandas, we first need to create a spark session or a spark context before we can import data. Spark Context is used as a channel to access all spark functionality. The spark driver program uses spark context to connect to the cluster through a resource manager (YARN or Mesos). SparkConf is required to create the spark context object, which stores configuration parameters like appName (to identify your spark driver), application, number of core and memory size of executor running on the worker node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/context.py:219: DeprecationWarning: Support for Python 2 and Python 3 prior to version 3.6 is deprecated as of Spark 3.0. See also the plan for dropping Python 2 support at https://spark.apache.org/news/plan-for-dropping-python-2-support.html.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "path = '/home/zhili/SparkSample/us_counties.csv' # defline the file path \n",
    "# Pandas \n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Pyspark \n",
    "appName = \"PySpark SQL Server Example - via JDBC\"\n",
    "master = \"local\"\n",
    "conf = pyspark.SparkConf().set('spark.driver.host','127.0.0.1').setAppName(appName).setMaster(master).set(\"spark.driver.extraClassPath\",\"sqljdbc_7.2/enu/mssql-jdbc-7.2.1.jre8.jar\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Our first Python Spark SQL example\") \\\n",
    "    .getOrCreate()\n",
    "df_s = spark.read.csv(path,header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\tDisplay the schema of the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date       object\n",
      "county     object\n",
      "state      object\n",
      "fips      float64\n",
      "cases       int64\n",
      "deaths      int64\n",
      "dtype: object\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fips: string (nullable = true)\n",
      " |-- cases: string (nullable = true)\n",
      " |-- deaths: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## pandas \n",
    "print(df.dtypes)\n",
    "## pyspark \n",
    "print(df_s.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Change the Data Types when Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we check the data types above, we found that the cases and deaths need to be converted to numerical values instead of string format. Here is the way how we can change the data types when importing the data in pandas and spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30843 entries, 0 to 30842\n",
      "Data columns (total 6 columns):\n",
      "date      30843 non-null object\n",
      "county    30843 non-null object\n",
      "state     30843 non-null object\n",
      "fips      30388 non-null float64\n",
      "cases     30843 non-null uint32\n",
      "deaths    30843 non-null uint32\n",
      "dtypes: float64(1), object(3), uint32(2)\n",
      "memory usage: 6.3 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# pandas : aim to reduce memory size \n",
    "\n",
    "types = {'date': 'object', 'county': 'object', \n",
    "         'state': 'object', 'fips': 'float',\n",
    "         'cases': np.uint32, 'deaths': np.uint32}\n",
    "\n",
    "df = pd.read_csv(path, usecols=types.keys(), dtype=types)\n",
    "print(df.info(memory_usage='deep'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, I change the data type of cases and deaths to np.uint32.  We can use 'integer' as the data type for these two columns, but np.unit 32 can help us to save memory. If your data is too large, you can also change the object data type to category to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fips: float (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "newDF=[StructField('date',StringType(),True),\n",
    "       StructField('county',StringType(),True),\n",
    "       StructField('state',StringType(),True),\n",
    "       StructField('fips',FloatType(),True),\n",
    "       StructField('cases',IntegerType(),True),\n",
    "       StructField('deaths',IntegerType(),True)\n",
    "       ]\n",
    "finalStruct=StructType(fields=newDF)\n",
    "df_s = spark.read.csv(path,header = True,schema=finalStruct)\n",
    "df_s.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Show the head of the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Cook</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date     county       state     fips  cases  deaths\n",
       "0  2020-01-21  Snohomish  Washington  53061.0      1       0\n",
       "1  2020-01-22  Snohomish  Washington  53061.0      1       0\n",
       "2  2020-01-23  Snohomish  Washington  53061.0      1       0\n",
       "3  2020-01-24       Cook    Illinois  17031.0      1       0\n",
       "4  2020-01-24  Snohomish  Washington  53061.0      1       0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-------+-----+------+\n",
      "|      date|   county|     state|   fips|cases|deaths|\n",
      "+----------+---------+----------+-------+-----+------+\n",
      "|2020-01-21|Snohomish|Washington|53061.0|    1|     0|\n",
      "|2020-01-22|Snohomish|Washington|53061.0|    1|     0|\n",
      "|2020-01-23|Snohomish|Washington|53061.0|    1|     0|\n",
      "|2020-01-24|     Cook|  Illinois|17031.0|    1|     0|\n",
      "|2020-01-24|Snohomish|Washington|53061.0|    1|     0|\n",
      "+----------+---------+----------+-------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(date='2020-01-21', county='Snohomish', state='Washington', fips=53061.0, cases=1, deaths=0, new_num='4'),\n",
       " Row(date='2020-01-22', county='Snohomish', state='Washington', fips=53061.0, cases=1, deaths=0, new_num='4'),\n",
       " Row(date='2020-01-23', county='Snohomish', state='Washington', fips=53061.0, cases=1, deaths=0, new_num='4'),\n",
       " Row(date='2020-01-24', county='Cook', state='Illinois', fips=17031.0, cases=1, deaths=0, new_num='4'),\n",
       " Row(date='2020-01-24', county='Snohomish', state='Washington', fips=53061.0, cases=1, deaths=0, new_num='4')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pyspark, take() and show() are different. show() prints results, take() returns a list of rows (in PySpark) and can be used to create a new dataframe. They are both actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.\tSelect Columns from the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        state  cases\n",
       "0  Washington      1\n",
       "1  Washington      1\n",
       "2  Washington      1\n",
       "3    Illinois      1\n",
       "4  Washington      1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Pandas \n",
    "df[['state','cases']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|     state|cases|\n",
      "+----------+-----+\n",
      "|Washington|    1|\n",
      "|Washington|    1|\n",
      "|Washington|    1|\n",
      "|  Illinois|    1|\n",
      "|Washington|    1|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## pyspark \n",
    "df_s.select('state','cases').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.\tShow the Statistics of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>new_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30843.000000</td>\n",
       "      <td>30843.000000</td>\n",
       "      <td>30843.000000</td>\n",
       "      <td>30843.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28817.802192</td>\n",
       "      <td>59.337937</td>\n",
       "      <td>1.151607</td>\n",
       "      <td>62.337937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15922.062074</td>\n",
       "      <td>778.748660</td>\n",
       "      <td>20.304794</td>\n",
       "      <td>778.748660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16069.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>28075.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>42079.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>56043.000000</td>\n",
       "      <td>57160.000000</td>\n",
       "      <td>1867.000000</td>\n",
       "      <td>57163.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fips         cases        deaths       new_num\n",
       "count  30843.000000  30843.000000  30843.000000  30843.000000\n",
       "mean   28817.802192     59.337937      1.151607     62.337937\n",
       "std    15922.062074    778.748660     20.304794    778.748660\n",
       "min       -1.000000      0.000000      0.000000      3.000000\n",
       "25%    16069.000000      1.000000      0.000000      4.000000\n",
       "50%    28075.000000      4.000000      0.000000      7.000000\n",
       "75%    42079.000000     14.000000      0.000000     17.000000\n",
       "max    56043.000000  57160.000000   1867.000000  57163.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df.describe() # show the stats for all numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    30843.000000\n",
       "mean        59.337937\n",
       "std        778.748660\n",
       "min          0.000000\n",
       "25%          1.000000\n",
       "50%          4.000000\n",
       "75%         14.000000\n",
       "max      57160.000000\n",
       "Name: cases, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cases'].describe() # show the stats for a specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|      date|   county|  state|              fips|             cases|            deaths|           new_num|\n",
      "+-------+----------+---------+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|     30843|    30843|  30843|             30388|             30843|             30843|             30843|\n",
      "|   mean|      null|     null|   null|29249.306568382257|59.337937295334434|1.1516065233602437|62.337937295334434|\n",
      "| stddev|      null|     null|   null|15642.441465721475|   778.74866005819|20.304793699175974|   778.74866005819|\n",
      "|    min|2020-01-21|Abbeville|Alabama|            1001.0|                 0|                 0|                10|\n",
      "|    max|2020-04-03|     Yuma|Wyoming|           56043.0|             57160|              1867|              9970|\n",
      "+-------+----------+---------+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s.describe().show() #pyspark can show stats for all columns but it may contain missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|             cases|\n",
      "+-------+------------------+\n",
      "|  count|             30843|\n",
      "|   mean|59.337937295334434|\n",
      "| stddev|   778.74866005819|\n",
      "|    min|                 0|\n",
      "|    max|             57160|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s.describe('cases').show() # show the stats for a specific column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Drop Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Washington\n",
       "3          Illinois\n",
       "5        California\n",
       "8           Arizona\n",
       "44    Massachusetts\n",
       "Name: state, dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df.state.drop_duplicates().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|        state|\n",
      "+-------------+\n",
      "|   Washington|\n",
      "|     Illinois|\n",
      "|   California|\n",
      "|      Arizona|\n",
      "|Massachusetts|\n",
      "+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s.select('state').dropDuplicates().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Missing Values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date        0\n",
       "county      0\n",
       "state       0\n",
       "fips      455\n",
       "cases       0\n",
       "deaths      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas \n",
    "df.isnull().sum() #check the number of missing value for each column \n",
    "# if we want to check the non-missing value we can use notnull() instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+----+-----+------+\n",
      "|date|county|state|fips|cases|deaths|\n",
      "+----+------+-----+----+-----+------+\n",
      "|   0|     0|    0| 455|    0|     0|\n",
      "+----+------+-----+----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_s.columns]).show()\n",
    "# if we want to check the non-missing value we can use isNotNull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql.functions.isnan(col): an expression that returns true iff the column is NaN.\n",
    "<br>\n",
    "isNull() :True if the current expression is null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Drop NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30388\n",
      "30843\n"
     ]
    }
   ],
   "source": [
    "# pandas \n",
    "df_valid = df.dropna(subset=['fips','state'], how ='any') #all \n",
    "print(df_valid.shape[0])\n",
    "print(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30388\n",
      "30843\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s_valid = df_s.dropna(how='any', subset =['fips', 'state']) #all\n",
    "print(df_s_valid.count())\n",
    "print(df_s.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Replace NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date      0\n",
      "county    0\n",
      "state     0\n",
      "fips      0\n",
      "cases     0\n",
      "deaths    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# pandas \n",
    "# Replace Missing values with 0 \n",
    "print(df.fillna(0).isnull().sum())\n",
    "# Replace Missing values based on specific columns \n",
    "values = {'fips': -1, 'cases': 0, 'deaths': 0}\n",
    "df.fillna(value=values, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+----+-----+------+\n",
      "|date|county|state|fips|cases|deaths|\n",
      "+----+------+-----+----+-----+------+\n",
      "|   0|     0|    0|   0|    0|     0|\n",
      "+----+------+-----+----+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date: string, county: string, state: string, fips: float, cases: int, deaths: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark \n",
    "# Replace Missing values with 0 \n",
    "df_s.na.fill(0).select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_s.columns]).show()\n",
    "# Replace Missing values based on specific columns \n",
    "df_s.fillna({'fips':'0','cases': 0, 'deaths': 0 })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Datetime Manipulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "3    4\n",
       "4    4\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df['date'] = pd.to_datetime(df.date) # change the data type from string to datetime \n",
    "# Extract weekday from the datetime column\n",
    "df.date.dt.weekday.head() # alternatively, we can use year, month, day,dayofweek ,second,hour, quarter instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- fips: float (nullable = true)\n",
      " |-- cases: integer (nullable = true)\n",
      " |-- deaths: integer (nullable = true)\n",
      " |-- record_date: date (nullable = true)\n",
      "\n",
      "+-------+--------+------+---------+-------+------+----------+----------+\n",
      "|dt_year|dt_month|dt_day|dt_dayofy|dt_hour|dt_min|dt_week_no|    dt_int|\n",
      "+-------+--------+------+---------+-------+------+----------+----------+\n",
      "|   2020|       1|    21|       21|      0|     0|         4|1579593600|\n",
      "|   2020|       1|    22|       22|      0|     0|         4|1579680000|\n",
      "|   2020|       1|    23|       23|      0|     0|         4|1579766400|\n",
      "|   2020|       1|    24|       24|      0|     0|         4|1579852800|\n",
      "|   2020|       1|    24|       24|      0|     0|         4|1579852800|\n",
      "|   2020|       1|    25|       25|      0|     0|         4|1579939200|\n",
      "|   2020|       1|    25|       25|      0|     0|         4|1579939200|\n",
      "|   2020|       1|    25|       25|      0|     0|         4|1579939200|\n",
      "|   2020|       1|    26|       26|      0|     0|         4|1580025600|\n",
      "|   2020|       1|    26|       26|      0|     0|         4|1580025600|\n",
      "|   2020|       1|    26|       26|      0|     0|         4|1580025600|\n",
      "|   2020|       1|    26|       26|      0|     0|         4|1580025600|\n",
      "|   2020|       1|    26|       26|      0|     0|         4|1580025600|\n",
      "|   2020|       1|    27|       27|      0|     0|         5|1580112000|\n",
      "|   2020|       1|    27|       27|      0|     0|         5|1580112000|\n",
      "|   2020|       1|    27|       27|      0|     0|         5|1580112000|\n",
      "|   2020|       1|    27|       27|      0|     0|         5|1580112000|\n",
      "|   2020|       1|    27|       27|      0|     0|         5|1580112000|\n",
      "|   2020|       1|    28|       28|      0|     0|         5|1580198400|\n",
      "|   2020|       1|    28|       28|      0|     0|         5|1580198400|\n",
      "+-------+--------+------+---------+-------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "# change the data type from string to datetime \n",
    "df_spark = df_s.withColumn(\"record_date\",df_s['date'].cast(DateType()))\n",
    "df_spark.printSchema() #show the current schema \n",
    "newdf = df_spark.select(year(df_spark.record_date).alias('dt_year'), month(df_spark.record_date).alias('dt_month'), dayofmonth(df_spark.record_date).alias('dt_day'), dayofyear(df_spark.record_date).alias('dt_dayofy'), hour(df_spark.record_date).alias('dt_hour'), minute(df_spark.record_date).alias('dt_min'), weekofyear(df_spark.record_date).alias('dt_week_no'), unix_timestamp(df_spark.record_date).alias('dt_int'))\n",
    "newdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use functions in pyspark.sql.functions: functions like year, month, etc. refer to here: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. \tFilter Data Based on Conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date       county       state    fips  cases  deaths  new_num\n",
      "5  2020-01-25       Orange  California  6059.0      1       0        4\n",
      "9  2020-01-26  Los Angeles  California  6037.0      1       0        4\n",
      "10 2020-01-26       Orange  California  6059.0      1       0        4\n",
      "14 2020-01-27  Los Angeles  California  6037.0      1       0        4\n",
      "15 2020-01-27       Orange  California  6059.0      1       0        4\n",
      "    cases  deaths\n",
      "5       1       0\n",
      "9       1       0\n",
      "10      1       0\n",
      "14      1       0\n",
      "15      1       0\n"
     ]
    }
   ],
   "source": [
    "# Pandas \n",
    "print(df[df.state =='California'].head()) # Filter data based on the state is CA\n",
    "print(df[df.state =='California'] [['cases','deaths']].head())  # only show specific columns based on the filtering condistions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+------+-----+------+-------+\n",
      "|      date|     county|     state|  fips|cases|deaths|new_num|\n",
      "+----------+-----------+----------+------+-----+------+-------+\n",
      "|2020-01-25|     Orange|California|6059.0|    1|     0|      4|\n",
      "|2020-01-26|Los Angeles|California|6037.0|    1|     0|      4|\n",
      "|2020-01-26|     Orange|California|6059.0|    1|     0|      4|\n",
      "|2020-01-27|Los Angeles|California|6037.0|    1|     0|      4|\n",
      "|2020-01-27|     Orange|California|6059.0|    1|     0|      4|\n",
      "+----------+-----------+----------+------+-----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----------+----------+------+-----+------+-------+\n",
      "|      date|     county|     state|  fips|cases|deaths|new_num|\n",
      "+----------+-----------+----------+------+-----+------+-------+\n",
      "|2020-01-25|     Orange|California|6059.0|    1|     0|      4|\n",
      "|2020-01-26|Los Angeles|California|6037.0|    1|     0|      4|\n",
      "|2020-01-26|     Orange|California|6059.0|    1|     0|      4|\n",
      "|2020-01-27|Los Angeles|California|6037.0|    1|     0|      4|\n",
      "|2020-01-27|     Orange|California|6059.0|    1|     0|      4|\n",
      "+----------+-----------+----------+------+-----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+-----+\n",
      "|deaths|cases|\n",
      "+------+-----+\n",
      "|     0|    1|\n",
      "|     0|    1|\n",
      "|     0|    1|\n",
      "|     0|    1|\n",
      "|     0|    1|\n",
      "+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s.where(df_s.state == 'California').show(5)\n",
    "# Alternatively, we can write it in this way\n",
    "df_s[df_s.state.isin(\"California\")].show(5)\n",
    "##only show specific columns based on the filtering condistions \n",
    "df_s.select('deaths', 'cases').filter(df_s.state == 'California').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\tGroup By with Aggregation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common aggreagation functions for both pandas and pyspark include: sum(), count(),mean(), min(),max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "state\n",
       "Alabama                      10042\n",
       "Alaska                        1296\n",
       "Arizona                      11647\n",
       "Arkansas                      6527\n",
       "California                   89145\n",
       "Colorado                     29895\n",
       "Connecticut                  26902\n",
       "Delaware                      3098\n",
       "District of Columbia          5314\n",
       "Florida                      62627\n",
       "Georgia                      38318\n",
       "Guam                          1047\n",
       "Hawaii                        2330\n",
       "Idaho                         4934\n",
       "Illinois                     57310\n",
       "Indiana                      19069\n",
       "Iowa                          4655\n",
       "Kansas                        4076\n",
       "Kentucky                      5653\n",
       "Louisiana                    54795\n",
       "Maine                         3278\n",
       "Maryland                     15871\n",
       "Massachusetts                61632\n",
       "Michigan                     71954\n",
       "Minnesota                     6706\n",
       "Mississippi                   9412\n",
       "Missouri                     11968\n",
       "Montana                       1922\n",
       "Nebraska                      2633\n",
       "Nevada                       11214\n",
       "New Hampshire                 3666\n",
       "New Jersey                  170274\n",
       "New Mexico                    3250\n",
       "New York                    743198\n",
       "North Carolina               14176\n",
       "North Dakota                  1269\n",
       "Northern Mariana Islands        30\n",
       "Ohio                         20878\n",
       "Oklahoma                      5657\n",
       "Oregon                        7086\n",
       "Pennsylvania                 45088\n",
       "Puerto Rico                   1975\n",
       "Rhode Island                  4875\n",
       "South Carolina               10876\n",
       "South Dakota                  1196\n",
       "Tennessee                    20943\n",
       "Texas                        35446\n",
       "Utah                          8778\n",
       "Vermont                       3048\n",
       "Virgin Islands                 335\n",
       "Virginia                     12151\n",
       "Washington                   63368\n",
       "West Virginia                 1516\n",
       "Wisconsin                    14576\n",
       "Wyoming                       1235\n",
       "Name: cases, dtype: uint32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df.groupby('state').cases.sum() # the total cases for each state \n",
    "# can change to min(),max(), mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|         state|sum(cases)|\n",
      "+--------------+----------+\n",
      "|    Washington|     63368|\n",
      "|      Illinois|     57310|\n",
      "|    California|     89145|\n",
      "|       Arizona|     11647|\n",
      "| Massachusetts|     61632|\n",
      "|     Wisconsin|     14576|\n",
      "|         Texas|     35446|\n",
      "|      Nebraska|      2633|\n",
      "|          Utah|      8778|\n",
      "|        Oregon|      7086|\n",
      "|       Florida|     62627|\n",
      "|      New York|    743198|\n",
      "|  Rhode Island|      4875|\n",
      "|       Georgia|     38318|\n",
      "| New Hampshire|      3666|\n",
      "|North Carolina|     14176|\n",
      "|    New Jersey|    170274|\n",
      "|      Colorado|     29895|\n",
      "|      Maryland|     15871|\n",
      "|        Nevada|     11214|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "df_s.groupby(df_s.state).agg(F.sum('cases')).show()\n",
    "#F.mean(), F.max(), F.countDistinct(), F.min(), F.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Alabama</th>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-14</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-15</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-16</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-17</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    cases  deaths\n",
       "state   date                     \n",
       "Alabama 2020-03-13      6       0\n",
       "        2020-03-14     12       0\n",
       "        2020-03-15     23       0\n",
       "        2020-03-16     29       0\n",
       "        2020-03-17     39       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "# apply different aggregation functions for different columns \n",
    "df.groupby(['state','date']).agg({'cases': 'sum', 'deaths': 'max'}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+\n",
      "|     state|      date|sum(cases)|max(deaths)|\n",
      "+----------+----------+----------+-----------+\n",
      "|Washington|2020-01-21|         1|          0|\n",
      "|Washington|2020-01-22|         1|          0|\n",
      "|Washington|2020-01-23|         1|          0|\n",
      "|  Illinois|2020-01-24|         1|          0|\n",
      "|Washington|2020-01-24|         1|          0|\n",
      "|California|2020-01-25|         1|          0|\n",
      "|  Illinois|2020-01-25|         1|          0|\n",
      "|Washington|2020-01-25|         1|          0|\n",
      "|   Arizona|2020-01-26|         1|          0|\n",
      "|California|2020-01-26|         2|          0|\n",
      "|  Illinois|2020-01-26|         1|          0|\n",
      "|Washington|2020-01-26|         1|          0|\n",
      "|   Arizona|2020-01-27|         1|          0|\n",
      "|California|2020-01-27|         2|          0|\n",
      "|  Illinois|2020-01-27|         1|          0|\n",
      "|Washington|2020-01-27|         1|          0|\n",
      "|   Arizona|2020-01-28|         1|          0|\n",
      "|California|2020-01-28|         2|          0|\n",
      "|  Illinois|2020-01-28|         1|          0|\n",
      "|Washington|2020-01-28|         1|          0|\n",
      "+----------+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark \n",
    "# apply different aggregation functions for different columns \n",
    "df_s.groupBy(['state','date']).agg({'cases': 'sum', 'deaths': 'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to compare the aggregation results, since the pandas dataframe and pyspark dataframe are in different orders. The following shows how can we sort the data frame based on specific columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sort Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we use sort_values(), while we use sort() in pyspark to sort the data frame based on specific columns. The default sorting order is ascending.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state       date  cases  deaths\n",
       "0  Alabama 2020-03-13      6       0\n",
       "1  Alabama 2020-03-14     12       0\n",
       "2  Alabama 2020-03-15     23       0\n",
       "3  Alabama 2020-03-16     29       0\n",
       "4  Alabama 2020-03-17     39       0\n",
       "5  Alabama 2020-03-18     51       0\n",
       "6  Alabama 2020-03-19     78       0\n",
       "7  Alabama 2020-03-20    106       0\n",
       "8  Alabama 2020-03-21    131       0\n",
       "9  Alabama 2020-03-22    157       0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df_agg = df.groupby(['state','date']).agg({'deaths': 'sum',\n",
    "                                 'cases': 'sum'}).reset_index().sort_values(by =['state', 'date'],\n",
    "                                                                              ascending = True)\n",
    "df_agg.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-----------+\n",
      "|  state|      date|sum(cases)|sum(deaths)|\n",
      "+-------+----------+----------+-----------+\n",
      "|Alabama|2020-03-13|         6|          0|\n",
      "|Alabama|2020-03-14|        12|          0|\n",
      "|Alabama|2020-03-15|        23|          0|\n",
      "|Alabama|2020-03-16|        29|          0|\n",
      "|Alabama|2020-03-17|        39|          0|\n",
      "|Alabama|2020-03-18|        51|          0|\n",
      "|Alabama|2020-03-19|        78|          0|\n",
      "|Alabama|2020-03-20|       106|          0|\n",
      "|Alabama|2020-03-21|       131|          0|\n",
      "|Alabama|2020-03-22|       157|          0|\n",
      "|Alabama|2020-03-23|       196|          0|\n",
      "|Alabama|2020-03-24|       242|          0|\n",
      "|Alabama|2020-03-25|       386|          1|\n",
      "|Alabama|2020-03-26|       538|          3|\n",
      "|Alabama|2020-03-27|       639|          4|\n",
      "|Alabama|2020-03-28|       720|          4|\n",
      "|Alabama|2020-03-29|       830|          5|\n",
      "|Alabama|2020-03-30|       947|         11|\n",
      "|Alabama|2020-03-31|       999|         14|\n",
      "|Alabama|2020-04-01|      1108|         28|\n",
      "+-------+----------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark\n",
    "df_s_agg = df_s.groupBy(['state','date']).agg({'deaths': 'sum', \n",
    "                                    'cases': 'sum'}).sort(['state','date'], ascending =True)\n",
    "df_s_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Rename Columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the aggregation functions, the names of some columns are not reasonable. We need to rename these column names to avoid confusion. The following shows how can we rename columns in pandas and pyspark dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>total_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     state       date  total_cases  total_death\n",
       "0  Alabama 2020-03-13            6            0\n",
       "1  Alabama 2020-03-14           12            0\n",
       "2  Alabama 2020-03-15           23            0\n",
       "3  Alabama 2020-03-16           29            0\n",
       "4  Alabama 2020-03-17           39            0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df_agg.rename(columns={\"deaths\": \"total_death\", \"cases\": \"total_cases\"}, inplace = True)\n",
    "df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+-----------+\n",
      "|  state|      date|total_cases|total_death|\n",
      "+-------+----------+-----------+-----------+\n",
      "|Alabama|2020-03-13|          6|          0|\n",
      "|Alabama|2020-03-14|         12|          0|\n",
      "|Alabama|2020-03-15|         23|          0|\n",
      "|Alabama|2020-03-16|         29|          0|\n",
      "|Alabama|2020-03-17|         39|          0|\n",
      "+-------+----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark\n",
    "df_s_agg = df_s_agg.withColumnRenamed(\"sum(cases)\",\"total_cases\").withColumnRenamed(\"sum(deaths)\", \"total_death\")\n",
    "df_s_agg.show(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Create a New Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>total_death</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>fata_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Florida</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>0.234043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Northern Mariana Islands</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Florida</td>\n",
       "      <td>2020-03-07</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         state        date  total_death  total_cases  \\\n",
       "1641                Washington  2020-03-03           10           32   \n",
       "322                    Florida  2020-03-06            2            7   \n",
       "1640                Washington  2020-03-02            6           23   \n",
       "1642                Washington  2020-03-04           11           47   \n",
       "1376              South Dakota  2020-03-10            1            5   \n",
       "587                     Kansas  2020-03-12            1            5   \n",
       "1639                Washington  2020-03-01            3           17   \n",
       "588                     Kansas  2020-03-13            1            6   \n",
       "1168  Northern Mariana Islands  2020-04-01            1            6   \n",
       "323                    Florida  2020-03-07            2           12   \n",
       "\n",
       "      fata_rate  \n",
       "1641   0.312500  \n",
       "322    0.285714  \n",
       "1640   0.260870  \n",
       "1642   0.234043  \n",
       "1376   0.200000  \n",
       "587    0.200000  \n",
       "1639   0.176471  \n",
       "588    0.166667  \n",
       "1168   0.166667  \n",
       "323    0.166667  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas\n",
    "# create a new column by dividing two columns \n",
    "df_agg['fata_rate'] = df_agg['total_death']/df_agg['total_cases'] # show the fatality rate \n",
    "df_agg.sort_values(by='fata_rate',ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+-----------+-------------------+\n",
      "|               state|      date|total_cases|total_death|          fata_rate|\n",
      "+--------------------+----------+-----------+-----------+-------------------+\n",
      "|          Washington|2020-03-03|         32|         10|             0.3125|\n",
      "|             Florida|2020-03-06|          7|          2| 0.2857142857142857|\n",
      "|          Washington|2020-03-02|         23|          6| 0.2608695652173913|\n",
      "|          Washington|2020-03-04|         47|         11|0.23404255319148937|\n",
      "|        South Dakota|2020-03-10|          5|          1|                0.2|\n",
      "|              Kansas|2020-03-12|          5|          1|                0.2|\n",
      "|          Washington|2020-03-01|         17|          3|0.17647058823529413|\n",
      "|             Florida|2020-03-07|         12|          2|0.16666666666666666|\n",
      "|              Kansas|2020-03-13|          6|          1|0.16666666666666666|\n",
      "|Northern Mariana ...|2020-04-01|          6|          1|0.16666666666666666|\n",
      "+--------------------+----------+-----------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pyspark\n",
    "# use withColumn to create a new column in pyspark \n",
    "df_s_agg = df_s_agg.withColumn('fata_rate', \n",
    "                    F.col('total_death')/F.col('total_cases')).sort('fata_rate',ascending =False)\n",
    "df_s_agg.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Join Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we can either use merge() or join to join two dataframe. My example shows how to use merge to join two tables. For pyspark, we use join() to join two dataframe. The default join for both data frame is inner join. We can change it to left join, right join or outer join by changing the parameter in how{left, right, outer, inner}. For more details, you can check in https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html &\n",
    "http://www.learnbymarketing.com/1100/pyspark-joins-by-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>total_cases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>15246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>15246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>15246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cook</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>41916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>15246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      county       state     fips  total_cases\n",
       "0  Snohomish  Washington  53061.0        15246\n",
       "1  Snohomish  Washington  53061.0        15246\n",
       "2  Snohomish  Washington  53061.0        15246\n",
       "3       Cook    Illinois  17031.0        41916\n",
       "4  Snohomish  Washington  53061.0        15246"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "# create a new dataframe by calculating the total case for each fip\n",
    "table_a = df.groupby('fips').cases.sum().reset_index() \n",
    "# rename the columns for the new dataframe \n",
    "table_a.columns = ['fips', 'total_cases']\n",
    "# create another dataframe by only extracting some columns from the original data frame \n",
    "table_b = df[['county','state','fips']]\n",
    "# Left Join two tables \n",
    "table_b.merge(table_a , on ='fips', how ='left').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+-------+-----------+\n",
      "|   county|     state|   fips|   fips|total_cases|\n",
      "+---------+----------+-------+-------+-----------+\n",
      "|Snohomish|Washington|53061.0|53061.0|      15246|\n",
      "|Snohomish|Washington|53061.0|53061.0|      15246|\n",
      "|Snohomish|Washington|53061.0|53061.0|      15246|\n",
      "|     Cook|  Illinois|17031.0|17031.0|      41916|\n",
      "|Snohomish|Washington|53061.0|53061.0|      15246|\n",
      "+---------+----------+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "TableA =  df_s.groupBy('fips').agg(F.sum('cases').alias('total_cases'))\n",
    "TableB = df_s.select(['county','state','fips'])\n",
    "ta = TableA.alias('ta') \n",
    "#The alias provides a short name for referencing fields and for referencing the fields after creation of the joined table.\n",
    "tb = TableB.alias('tb')\n",
    "tb.join(ta, ta.fips==tb.fips, how ='left' ).show(5) # Could also use 'left_outer'\n",
    "# right, right_outer, full, default inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. User Defined Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Defined Functions (aka UDF) is a feature of Spark SQL to define new Column-based functions that extend the vocabulary of Spark SQL's DSL for transforming Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+-------+-----+------+-------+\n",
      "|      date|   county|     state|   fips|cases|deaths|new_num|\n",
      "+----------+---------+----------+-------+-----+------+-------+\n",
      "|2020-01-21|Snohomish|Washington|53061.0|    1|     0|      4|\n",
      "|2020-01-22|Snohomish|Washington|53061.0|    1|     0|      4|\n",
      "|2020-01-23|Snohomish|Washington|53061.0|    1|     0|      4|\n",
      "|2020-01-24|     Cook|  Illinois|17031.0|    1|     0|      4|\n",
      "|2020-01-24|Snohomish|Washington|53061.0|    1|     0|      4|\n",
      "+----------+---------+----------+-------+-----+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "add_number = udf(lambda x: x+3)\n",
    "df_s = df_s.withColumn(\"new_num\", add_number(df_s.cases))\n",
    "df_s.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>fips</th>\n",
       "      <th>cases</th>\n",
       "      <th>deaths</th>\n",
       "      <th>new_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Cook</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>17031.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>Snohomish</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53061.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     county       state     fips  cases  deaths  new_num\n",
       "0 2020-01-21  Snohomish  Washington  53061.0      1       0        4\n",
       "1 2020-01-22  Snohomish  Washington  53061.0      1       0        4\n",
       "2 2020-01-23  Snohomish  Washington  53061.0      1       0        4\n",
       "3 2020-01-24       Cook    Illinois  17031.0      1       0        4\n",
       "4 2020-01-24  Snohomish  Washington  53061.0      1       0        4"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandas \n",
    "df['new_num']= df.cases.apply(lambda x: x+3)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Window Function to calculate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window function can be one of any of the standard aggregate functions (sum, count, max, min, avg) as well as a number of functions that can only be used as analytic functions. Window function can help us to write a query with less complexity. It's mainly composed of four main parts:\n",
    "1. The over() caluse: This tells the databaseto expect a window function, rather than a standard aggregate function.\n",
    "2. The partitionBy() clause: This clause tells the database how to break up the data. In other words, it is similar to a Group By in that it tells the database that row with the same values should be treated as a single group or parition.\n",
    "3. The orderBy() clause: It tells the database how to sort the data within each partition.\n",
    "4. The rangeBetween() clause: It defines the regions over which the function is calculated. \n",
    "      - unboundedPreceding: from the start of the partition \n",
    "      - unboundedFollowing: to the end of the partition \n",
    "      - 0: current row "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following exmaple shows how can we caluclate the cumulative cases for each state. Since we want to get the value for each state, so the data should be partitioned by the state. We want to get the cumulative number, so we frist need to sort the data within each state by date in and ascending order. Then,we need to define the window frame from the start of partition to the current row. Finally, we could apply the aggregation function 'sum' over this window function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+----------------+-----------------+\n",
      "|  state|      date|total_cases|cumulative_cases|cumulative_deaths|\n",
      "+-------+----------+-----------+----------------+-----------------+\n",
      "|Alabama|2020-03-13|          6|               6|                0|\n",
      "|Alabama|2020-03-14|         12|              18|                0|\n",
      "|Alabama|2020-03-15|         23|              41|                0|\n",
      "|Alabama|2020-03-16|         29|              70|                0|\n",
      "|Alabama|2020-03-17|         39|             109|                0|\n",
      "|Alabama|2020-03-18|         51|             160|                0|\n",
      "|Alabama|2020-03-19|         78|             238|                0|\n",
      "|Alabama|2020-03-20|        106|             344|                0|\n",
      "|Alabama|2020-03-21|        131|             475|                0|\n",
      "|Alabama|2020-03-22|        157|             632|                0|\n",
      "|Alabama|2020-03-23|        196|             828|                0|\n",
      "|Alabama|2020-03-24|        242|            1070|                0|\n",
      "|Alabama|2020-03-25|        386|            1456|                1|\n",
      "|Alabama|2020-03-26|        538|            1994|                4|\n",
      "|Alabama|2020-03-27|        639|            2633|                8|\n",
      "|Alabama|2020-03-28|        720|            3353|               12|\n",
      "|Alabama|2020-03-29|        830|            4183|               17|\n",
      "|Alabama|2020-03-30|        947|            5130|               28|\n",
      "|Alabama|2020-03-31|        999|            6129|               42|\n",
      "|Alabama|2020-04-01|       1108|            7237|               70|\n",
      "+-------+----------+-----------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "w = Window \\\n",
    "    .partitionBy('state') \\\n",
    "    .orderBy(asc('date')) \\\n",
    "    .rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df2 = df_s_agg.withColumn('cumulative_cases', \n",
    "                      F.sum(df_s_agg.total_cases).over(w))\\\n",
    "                        .withColumn('cumulative_deaths',F.sum(df_s_agg.total_death).over(w))\\\n",
    "                        .select(['state','date','total_cases','cumulative_cases', 'cumulative_deaths'])\\\n",
    "                        .orderBy('state')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>total_death</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>fata_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>0</td>\n",
       "      <td>344</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>0</td>\n",
       "      <td>475</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>0</td>\n",
       "      <td>828</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>0</td>\n",
       "      <td>1070</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>1</td>\n",
       "      <td>1456</td>\n",
       "      <td>0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>4</td>\n",
       "      <td>1994</td>\n",
       "      <td>0.008167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>8</td>\n",
       "      <td>2633</td>\n",
       "      <td>0.014427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>12</td>\n",
       "      <td>3353</td>\n",
       "      <td>0.019982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>17</td>\n",
       "      <td>4183</td>\n",
       "      <td>0.026006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>28</td>\n",
       "      <td>5130</td>\n",
       "      <td>0.037622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>42</td>\n",
       "      <td>6129</td>\n",
       "      <td>0.051636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>70</td>\n",
       "      <td>7237</td>\n",
       "      <td>0.076907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>102</td>\n",
       "      <td>8507</td>\n",
       "      <td>0.102104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>140</td>\n",
       "      <td>10042</td>\n",
       "      <td>0.126859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>90</td>\n",
       "      <td>6758</td>\n",
       "      <td>0.147051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1745</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>110</td>\n",
       "      <td>8027</td>\n",
       "      <td>0.162812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>135</td>\n",
       "      <td>9378</td>\n",
       "      <td>0.181316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>168</td>\n",
       "      <td>10930</td>\n",
       "      <td>0.202579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>205</td>\n",
       "      <td>12660</td>\n",
       "      <td>0.223967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>255</td>\n",
       "      <td>14576</td>\n",
       "      <td>0.250063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-13</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-16</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1757</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-18</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1758</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-19</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-20</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-21</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-23</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-25</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>0</td>\n",
       "      <td>386</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1767</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-28</td>\n",
       "      <td>0</td>\n",
       "      <td>471</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>0</td>\n",
       "      <td>559</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1769</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>0</td>\n",
       "      <td>655</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>776</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1771</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>0</td>\n",
       "      <td>914</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1772</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-04-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1068</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>0</td>\n",
       "      <td>1235</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1774 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          state        date  total_death  total_cases  fata_rate\n",
       "0       Alabama  2020-03-13            0            6   0.000000\n",
       "1       Alabama  2020-03-14            0           18   0.000000\n",
       "2       Alabama  2020-03-15            0           41   0.000000\n",
       "3       Alabama  2020-03-16            0           70   0.000000\n",
       "4       Alabama  2020-03-17            0          109   0.000000\n",
       "5       Alabama  2020-03-18            0          160   0.000000\n",
       "6       Alabama  2020-03-19            0          238   0.000000\n",
       "7       Alabama  2020-03-20            0          344   0.000000\n",
       "8       Alabama  2020-03-21            0          475   0.000000\n",
       "9       Alabama  2020-03-22            0          632   0.000000\n",
       "10      Alabama  2020-03-23            0          828   0.000000\n",
       "11      Alabama  2020-03-24            0         1070   0.000000\n",
       "12      Alabama  2020-03-25            1         1456   0.002591\n",
       "13      Alabama  2020-03-26            4         1994   0.008167\n",
       "14      Alabama  2020-03-27            8         2633   0.014427\n",
       "15      Alabama  2020-03-28           12         3353   0.019982\n",
       "16      Alabama  2020-03-29           17         4183   0.026006\n",
       "17      Alabama  2020-03-30           28         5130   0.037622\n",
       "18      Alabama  2020-03-31           42         6129   0.051636\n",
       "19      Alabama  2020-04-01           70         7237   0.076907\n",
       "20      Alabama  2020-04-02          102         8507   0.102104\n",
       "21      Alabama  2020-04-03          140        10042   0.126859\n",
       "22       Alaska  2020-03-12            0            1   0.000000\n",
       "23       Alaska  2020-03-13            0            2   0.000000\n",
       "24       Alaska  2020-03-14            0            3   0.000000\n",
       "25       Alaska  2020-03-15            0            4   0.000000\n",
       "26       Alaska  2020-03-16            0            7   0.000000\n",
       "27       Alaska  2020-03-17            0           13   0.000000\n",
       "28       Alaska  2020-03-18            0           22   0.000000\n",
       "29       Alaska  2020-03-19            0           34   0.000000\n",
       "...         ...         ...          ...          ...        ...\n",
       "1744  Wisconsin  2020-03-29           90         6758   0.147051\n",
       "1745  Wisconsin  2020-03-30          110         8027   0.162812\n",
       "1746  Wisconsin  2020-03-31          135         9378   0.181316\n",
       "1747  Wisconsin  2020-04-01          168        10930   0.202579\n",
       "1748  Wisconsin  2020-04-02          205        12660   0.223967\n",
       "1749  Wisconsin  2020-04-03          255        14576   0.250063\n",
       "1750    Wyoming  2020-03-11            0            1   0.000000\n",
       "1751    Wyoming  2020-03-12            0            2   0.000000\n",
       "1752    Wyoming  2020-03-13            0            4   0.000000\n",
       "1753    Wyoming  2020-03-14            0            7   0.000000\n",
       "1754    Wyoming  2020-03-15            0           10   0.000000\n",
       "1755    Wyoming  2020-03-16            0           20   0.000000\n",
       "1756    Wyoming  2020-03-17            0           35   0.000000\n",
       "1757    Wyoming  2020-03-18            0           52   0.000000\n",
       "1758    Wyoming  2020-03-19            0           70   0.000000\n",
       "1759    Wyoming  2020-03-20            0           92   0.000000\n",
       "1760    Wyoming  2020-03-21            0          116   0.000000\n",
       "1761    Wyoming  2020-03-22            0          142   0.000000\n",
       "1762    Wyoming  2020-03-23            0          170   0.000000\n",
       "1763    Wyoming  2020-03-24            0          207   0.000000\n",
       "1764    Wyoming  2020-03-25            0          256   0.000000\n",
       "1765    Wyoming  2020-03-26            0          312   0.000000\n",
       "1766    Wyoming  2020-03-27            0          386   0.000000\n",
       "1767    Wyoming  2020-03-28            0          471   0.000000\n",
       "1768    Wyoming  2020-03-29            0          559   0.000000\n",
       "1769    Wyoming  2020-03-30            0          655   0.000000\n",
       "1770    Wyoming  2020-03-31            0          776   0.000000\n",
       "1771    Wyoming  2020-04-01            0          914   0.000000\n",
       "1772    Wyoming  2020-04-02            0         1068   0.000000\n",
       "1773    Wyoming  2020-04-03            0         1235   0.000000\n",
       "\n",
       "[1774 rows x 5 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## pandas \n",
    "df_agg.groupby(['state', 'date']).sum().groupby(level=0).cumsum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               NaN\n",
       "1               NaN\n",
       "2         13.666667\n",
       "3         21.333333\n",
       "4         30.333333\n",
       "5         39.666667\n",
       "6         56.000000\n",
       "7         78.333333\n",
       "8        105.000000\n",
       "9        131.333333\n",
       "10       161.333333\n",
       "11       198.333333\n",
       "12       274.666667\n",
       "13       388.666667\n",
       "14       521.000000\n",
       "15       632.333333\n",
       "16       729.666667\n",
       "17       832.333333\n",
       "18       925.333333\n",
       "19      1018.000000\n",
       "20      1125.666667\n",
       "21      1304.333333\n",
       "22       935.333333\n",
       "23       512.333333\n",
       "24         1.000000\n",
       "25         1.000000\n",
       "26         1.666667\n",
       "27         3.333333\n",
       "28         6.000000\n",
       "29         9.000000\n",
       "           ...     \n",
       "1744    1030.666667\n",
       "1745    1143.666667\n",
       "1746    1246.666667\n",
       "1747    1390.666667\n",
       "1748    1544.333333\n",
       "1749    1732.666667\n",
       "1750    1215.666667\n",
       "1751     639.333333\n",
       "1752       1.333333\n",
       "1753       2.000000\n",
       "1754       2.666667\n",
       "1755       5.333333\n",
       "1756       9.333333\n",
       "1757      14.000000\n",
       "1758      16.666667\n",
       "1759      19.000000\n",
       "1760      21.333333\n",
       "1761      24.000000\n",
       "1762      26.000000\n",
       "1763      30.333333\n",
       "1764      38.000000\n",
       "1765      47.333333\n",
       "1766      59.666667\n",
       "1767      71.666667\n",
       "1768      82.333333\n",
       "1769      89.666667\n",
       "1770     101.666667\n",
       "1771     118.333333\n",
       "1772     137.666667\n",
       "1773     153.000000\n",
       "Name: total_cases, Length: 1774, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agg.total_cases.rolling(window =3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18.\tOperate SQL Queries within DataFrame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark comes with a sql library that lets us query Dataframe using the SQL syntax. At first, we have to create a temporary view of the DataFrame using createOrReplaceTempView(), which is a temporary SQL table. This will be enable us to use the same SQL syntax as we were using a database like Mysql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1302|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark \n",
    "df_s.createOrReplaceTempView(\"log_table\")\n",
    "spark.sql(\"select count(1) from log_table where state ='California'\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas cannot let us directly write sql queries within DataFrame, but we still can use query() to write some sql like syntax to manipulate the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date       1302\n",
       "county     1302\n",
       "state      1302\n",
       "fips       1302\n",
       "cases      1302\n",
       "deaths     1302\n",
       "new_num    1302\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pandas \n",
    "# query is to query the columns of a frame with a boolean expression.\n",
    "df.query(\"state=='California'\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Convert One Type of DataFrame to Another "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.1 Convert pandas to pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------+-----------+\n",
      "|  state|               date|total_cases|total_death|\n",
      "+-------+-------------------+-----------+-----------+\n",
      "|Alabama|2020-03-13 00:00:00|          6|          0|\n",
      "|Alabama|2020-03-14 00:00:00|         12|          0|\n",
      "|Alabama|2020-03-15 00:00:00|         23|          0|\n",
      "|Alabama|2020-03-16 00:00:00|         29|          0|\n",
      "|Alabama|2020-03-17 00:00:00|         39|          0|\n",
      "+-------+-------------------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pandas to pyspark \n",
    "df_s_agg_2 = spark.createDataFrame(df_agg)\n",
    "df_s_agg_2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use Pandas dataframes when using Spark, by calling toPandas() on a Spark dataframe, which returns a pandas object. However, this function should generally be avoided except when working with small dataframes, because it pulls the entire object into memory on a single node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19.2 Convert pyspark to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>date</th>\n",
       "      <th>total_cases</th>\n",
       "      <th>total_death</th>\n",
       "      <th>fata_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-03</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.312500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Florida</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-02</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0.260870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Washington</td>\n",
       "      <td>2020-03-04</td>\n",
       "      <td>47</td>\n",
       "      <td>11</td>\n",
       "      <td>0.234043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>2020-03-10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          state        date  total_cases  total_death  fata_rate\n",
       "0    Washington  2020-03-03           32           10   0.312500\n",
       "1       Florida  2020-03-06            7            2   0.285714\n",
       "2    Washington  2020-03-02           23            6   0.260870\n",
       "3    Washington  2020-03-04           47           11   0.234043\n",
       "4  South Dakota  2020-03-10            5            1   0.200000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s_agg.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20.\tExport the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data as csv file \n",
    "# pandas \n",
    "df_agg.to_csv('pd.csv')\n",
    "# pyspark \n",
    "df_s_agg.write.format('com.databricks.spark.csv').save(\"pyspark.csv\",header = 'true')\n",
    "# alternatively we can save pyspark to csv in this way if Spark 2.0+ \n",
    "df_s_agg.write.csv('pyspark2.csv',header ='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must stop() the active SparkContext before creating a new one. When we are done with our data manipulation, we need to run sc.stop() to end the active SparkContext. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
